# RDF vs Markdown LLM Agent Experiment

## Hypothesis

**LLM agents will produce more reliable insights when querying structured RDF data compared to processing raw markdown documents.**

## Experiment Overview

This experiment compares the reliability of LLM agent responses when extracting insights from:
- **Format A**: Markdown tables and prose (unstructured)
- **Format B**: RDF/Turtle with SPARQL queries (structured)

Both formats contain identical e-commerce product data (250 products across 6 categories).

## Setup Complete

All infrastructure is ready to run the experiment:

### ✅ Data Generated
- **250 products** across 6 categories (Electronics, Clothing, Home & Garden, Sports, Books, Toys)
- **40 brands** with realistic product names and descriptions
- **Edge cases**: 20 out-of-stock items, 19 missing descriptions
- **Price range**: $12.34 - $2,988.25

### ✅ Files Created

```
experiment/
├── data/
│   ├── products-master.json      # Source of truth (250 products)
│   ├── products.md                # Markdown format
│   ├── products.ttl               # RDF Turtle format (813 triples)
│   ├── queries.json               # 30 test queries
│   └── ground-truth.json          # Correct answers from SPARQL
├── scripts/
│   ├── generate-products.js       # Data generator
│   ├── generate-rdf.js            # JSON to Turtle converter
│   └── generate-ground-truth.sh   # SPARQL query runner
└── results/
    └── response-tracking.csv      # Template for recording results

.opencode/skills/
└── sparql-query-agent/            # Minimal SPARQL guidance skill
```

### ✅ Tools Installed
- **Comunica SPARQL** - Local SPARQL query engine for RDF data
- **Node.js scripts** - Data generation and conversion

---

## Running the Experiment

### Phase 1: Test Markdown Format (90 queries)

**For each of the 30 queries, run 3 times:**

1. **Start a fresh agent session**
   
2. **Provide markdown context:**
   ```
   I have product data in markdown format. I'll provide the data file, 
   then ask you questions about it.
   ```
   
3. **Share the file:** `experiment/data/products.md`

4. **Ask the question** (from `queries.json`):
   ```
   What is the price of product P001?
   ```

5. **Record response:**
   - Copy agent's answer to `response-tracking.csv`
   - Note timestamp
   - Mark if correct (check against `ground-truth.json`)
   - Add any observations (e.g., "agent counted manually", "missed some results")

6. **Repeat** for runs 2 and 3

### Phase 2: Test RDF Format (90 queries)

**For each of the 30 queries, run 3 times:**

1. **Start a fresh agent session**

2. **Load the SPARQL skill:**
   ```
   Load the sparql-query-agent skill. I have RDF product data 
   and want you to query it using SPARQL.
   ```

3. **Provide RDF context:**
   ```
   The data is in experiment/data/products.ttl
   
   You can query it using:
   npx comunica-sparql-file experiment/data/products.ttl -q "YOUR_QUERY"
   ```

4. **Ask the question** (same as markdown test):
   ```
   What is the price of product P001?
   ```

5. **Record response:**
   - The agent should construct a SPARQL query
   - Execute it via Comunica
   - Parse results and answer
   - Record in `response-tracking.csv`
   - Mark correctness
   - Note any issues (e.g., "SPARQL syntax error", "correct query, wrong parsing")

6. **Repeat** for runs 2 and 3

---

## Query Suite Breakdown

### Simple Lookups (Q001-Q010)
- Find specific product attributes (price, name, brand)
- Filter by category or brand
- Pattern matching (SKU ranges)
- Stock status checks

**Example:**
- Q001: "What is the price of product P001?"
- Q003: "List all products from the brand Sony."

### Aggregations (Q011-Q020)
- Count products
- Calculate averages, min, max
- Sum values (inventory value)
- Group by category/brand

**Example:**
- Q012: "What is the average price of all products?"
- Q015: "What is the total inventory value?"

### Relationship Traversal (Q021-Q030)
- Find related products
- Products sharing attributes (same brand, category)
- Multi-hop relationships
- Complex filters with grouping

**Example:**
- Q021: "What products are related to product P001?"
- Q024: "Which brands make products in both Electronics and Clothing?"

---

## Validation & Ground Truth

### Check Your Answers

All correct answers are in `experiment/data/ground-truth.json`, generated by running actual SPARQL queries.

**Example for Q001:**
```json
{
  "query_id": "Q001",
  "question": "What is the price of product P001?",
  "result": [{"price": "\"192.06\"^^http://www.w3.org/2001/XMLSchema#decimal"}]
}
```

**Expected answer:** $192.06 or 192.06

### Validation Tips

- **Exact matches**: Prices, product IDs, counts should be exact
- **List completeness**: Did agent find all items?
- **Format variance**: "192.06" vs "$192.06" vs "192.06 USD" are all correct
- **Rounding**: Allow 2 decimal places for averages
- **Hallucination**: Mark as WRONG if agent invents product IDs or data

---

## Quick Test (Validation)

Before running the full experiment, verify everything works:

### Test Markdown:
```
Question: How many total products are in the catalog?
Expected: 250
```

### Test RDF:
```
Question: How many total products are in the catalog?
Expected SPARQL:
SELECT (COUNT(?product) AS ?total) WHERE { ?product a schema:Product }

Expected result: 250
```

**Run this now:**
```bash
npx comunica-sparql-file experiment/data/products.ttl \
  -q "SELECT (COUNT(?product) AS ?total) WHERE { ?product a schema:Product }"
```

Should return: `[{"total":"\"250\"^^..."}]`

---

## Analysis After Experiment

### Calculate Metrics

From your `response-tracking.csv`:

1. **Accuracy**: % correct answers per format
2. **Consistency**: Did all 3 runs give same answer?
3. **By query type**: Which format struggled with aggregations vs lookups?
4. **Hallucination rate**: How often did agent invent data?
5. **Time**: Which was faster? (optional to track)

### Expected Patterns

**RDF might win because:**
- Structured queries prevent ambiguity
- SPARQL enforces logic
- No parsing errors
- Clear relationship traversal

**Markdown might surprise because:**
- LLMs trained heavily on markdown
- Natural language flexibility
- Better at fuzzy matching
- Easier to "see" patterns

---

## File Reference

### Key Data Files

**Products Markdown** (`experiment/data/products.md`)
- Human-readable tables organized by category
- 3,411 lines total
- Use for Markdown format tests

**Products RDF** (`experiment/data/products.ttl`)
- 813 RDF triples in Turtle format
- Schema.org vocabulary
- 121 KB file size
- Use for RDF format tests

**Queries** (`experiment/data/queries.json`)
- 30 questions with SPARQL hints
- Organized by type (lookup, aggregation, traversal)
- Includes expected answer types

**Ground Truth** (`experiment/data/ground-truth.json`)
- Correct answers from SPARQL execution
- Use for validation

### Helper Scripts

**Regenerate Data:**
```bash
node experiment/scripts/generate-products.js
node experiment/scripts/generate-rdf.js
./experiment/scripts/generate-ground-truth.sh
```

**Test SPARQL:**
```bash
npx comunica-sparql-file experiment/data/products.ttl -q "SELECT * WHERE { ?s ?p ?o } LIMIT 5"
```

---

## Statistics

- **Total queries to run**: 180 (30 questions × 3 runs × 2 formats)
- **Estimated time**: 3-4 hours for manual testing
- **Products**: 250
- **Brands**: 40
- **Categories**: 6 main, 30 subcategories
- **RDF triples**: 813
- **Out of stock**: 20 products
- **Price range**: $12.34 - $2,988.25

---

## Notes

- Keep agent sessions fresh (don't reuse context between runs)
- For Markdown: Agent can read the file, search text, parse tables
- For RDF: Agent should construct SPARQL, execute via Comunica, parse JSON results
- Record everything in `response-tracking.csv`
- Look for patterns: Does RDF fail on certain query types? Does Markdown hallucinate?

---

## Next Steps

1. **Validate setup** - Run the quick test above
2. **Start with 5 queries** - Get comfortable with the process
3. **Run full experiment** - All 30 queries × 3 runs × 2 formats
4. **Analyze results** - Calculate accuracy, consistency, error patterns
5. **Draw conclusions** - Was your hypothesis correct?

Good luck with your experiment!
